x-scaling:
  collectors-python: &scale-py-collectors
    scale: ${COLLECTORS_PY_SCALE:-1}
  collectors-java-cpc: &scale-java-cpc-collectors
    scale: ${COLLECTORS_JAVA_CPC_SCALE:-1}
  extractor: &scale-extractor
    scale: ${EXTRACTOR_SCALE:-1}
  merger: &scale-merger
    scale: ${MERGER_SCALE:-1}

x-base-service-definitions: 
  python: &python-collector
    <<: *scale-py-collectors
    restart: always
    depends_on:
      initializer:
        condition: service_completed_successfully
    networks:
      - kafka-clients
      - collectors

  java-cpc: &java-cpc-collector
    <<: *scale-java-cpc-collectors
    restart: always
    image: domrad/java-standalone
    entrypoint: [ "java", "-Dlog4j2.configurationFile=file:log4j2.xml", "-cp", "/app/domainradar-collector.jar", "cz.vut.fit.domainradar.standalone.StandaloneCollectorRunner" ]
    depends_on:
      initializer:
        condition: service_completed_successfully
    networks:
      - kafka-clients
      - collectors
   
services:
  kafka1:
    image: docker.io/apache/kafka:3.7.0
    hostname: kafka1
    volumes:
      - kafka1-single-log:/var/lib/kafka/data
    secrets:
      - kafka-truststore
      - kafka1-keystore
    networks:
      kafka-clients:
        ipv4_address: 192.168.100.2
        aliases: [ "kafka1" ]
      kafka-outside-world:
        ipv4_address: 192.168.103.250
    ports:
      - "31013:31013"
    env_file:
      - ./envs/kafka1_single.env

  # Kafka Connect in the full configuration, including PostgreSQL
  kafka-connect-full: &connect-base
    profiles:
      - full
    build:
      context: .
      dockerfile: ./dockerfiles/kafka_connect.Dockerfile
    depends_on:
      initializer:
        condition: service_completed_successfully
      postgres:
        condition: service_started
      mongo:
        condition: service_started
    secrets:
      - kafka-truststore
      - client1-keystore
    networks:
      kafka-clients:
        ipv4_address: 192.168.100.10
        aliases: [ "kafka-connect" ]
      databases: {}
      outside-world:
        ipv4_address: 192.168.103.242
    ports:
      - "31002:8083"
    volumes:
      - ./connect_properties/:/opt/kafka-connect/config/
      - kafka-connect-offsets:/tmp/kafka-connect

  # Kafka Connect without PostgreSQL, for use in the standalone collection scenarios
  kafka-connect-without-postgres:
    <<: *connect-base
    profiles:
      - colext
      - col
    command: ["/bin/bash", "-c", "/opt/kafka/bin/connect-standalone.sh /opt/kafka-connect/config/1*.properties /opt/kafka-connect/config/2*.properties"]
    depends_on:
      initializer:
        condition: service_completed_successfully
      mongo:
        condition: service_started

  kafka-ui:
    image: docker.io/provectuslabs/kafka-ui:latest
    depends_on:
      - kafka1
    secrets:
      - kafka-truststore
      - client1-keystore
    networks:
      kafka-clients:
        ipv4_address: 192.168.100.11
      outside-world:
        ipv4_address: 192.168.103.243
    ports:
      - "31000:8080"
    volumes:
      - ./client_properties/kafka_ui.yml:/etc/kafkaui/dynamic_config.yaml
    env_file:
      - ./envs/kafka_ui.env
    environment:
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=${BOOTSTRAP_SERVERS:-kafka1:9093}

  initializer:
    build:
      context: .
      dockerfile: ./dockerfiles/initializer.Dockerfile
    depends_on:
      - kafka1
    secrets:
      - kafka-truststore
      - client1-keystore
    networks:
      - kafka-clients
    volumes:
      - ./client_properties/initializer.properties:/scripts/client.properties
    environment:
      - BOOTSTRAP=${BOOTSTRAP_SERVERS:-kafka1:9093}
      - COMMAND_CONFIG_FILE=/scripts/client.properties
      - EXTRA_SLEEP=0
      - UPDATE_EXISTING_TOPICS=0

  collector-zone:
    <<: *python-collector
    image: domrad/zone
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    volumes:
      - ./client_properties/zone.toml:/app/config.toml
    
  collector-dns:
    <<: *python-collector
    image: domrad/dns
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    volumes:
      - ./client_properties/dns.toml:/app/config.toml

  collector-tls:
    <<: *java-cpc-collector
    entrypoint: [ "java", "-Djava.security.properties=/app/legacy.security", "-Dlog4j2.configurationFile=file:log4j2.xml", "-cp", "/app/domainradar-collector.jar", "cz.vut.fit.domainradar.standalone.StandaloneCollectorRunner" ]
    command: [ "--col-tls", "-id", "${ID_PREFIX-domrad}", "-p", "/app/client.properties", "-s", "${BOOTSTRAP_SERVERS:-kafka1:9093}" ]
    secrets:
      - kafka-truststore
      - client2-keystore
    volumes:
      - ./client_properties/tls.properties:/app/client.properties
      - ./client_properties/log4j2.xml:/app/log4j2.xml

  collector-nerd:
    <<: *java-cpc-collector
    command: [ "--col-nerd", "-id", "${ID_PREFIX-domrad}", "-p", "/app/client.properties", "-s", "${BOOTSTRAP_SERVERS:-kafka1:9093}" ]
    secrets:
      - kafka-truststore
      - client2-keystore
    volumes:
      - ./client_properties/nerd.properties:/app/client.properties
      - ./client_properties/log4j2.xml:/app/log4j2.xml

  collector-geoip:
    <<: *java-cpc-collector
    command: [ "--col-geo", "-id", "${ID_PREFIX-domrad}", "-p", "/app/client.properties", "-s", "${BOOTSTRAP_SERVERS:-kafka1:9093}" ]
    secrets:
      - kafka-truststore
      - client2-keystore
    volumes:
      - ./geoip_data/:/app/geoip/
      - ./client_properties/geoip.properties:/app/client.properties
      - ./client_properties/log4j2.xml:/app/log4j2.xml

  collector-rdap-dn:
    <<: *python-collector
    image: domrad/rdap-dn
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    volumes:
      - ./client_properties/rdap_dn.toml:/app/config.toml
    
  collector-rdap-ip:
    <<: *python-collector
    image: domrad/rdap-ip
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    volumes:
      - ./client_properties/rdap_ip.toml:/app/config.toml

  collector-rtt:
    <<: *python-collector
    image: domrad/rtt
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    volumes:
      - ./client_properties/rtt.toml:/app/config.toml
    cap_add:
      - CAP_NET_RAW
      - CAP_NET_ADMIN

  merger:
    <<: *scale-merger
    profiles:
      - colext
      - full
    image: domrad/java-streams
    entrypoint: "/bin/bash -c"
    # use $$HOSTNAME to differentiate between multiple instances - this is not nice but the other solution (without Swarm) is to duplicate the service manually...
    command: "\"/opt/java/openjdk/bin/java -Dlog4j2.configurationFile=file:log4j2.xml -cp /app/domainradar-collector.jar cz.vut.fit.domainradar.streams.StreamsPipelineRunner --merger -id ${ID_PREFIX-domrad}-merger -p /app/client.properties -s ${BOOTSTRAP_SERVERS:-kafka1:9093} -o state.dir=/app/streams-state/$$HOSTNAME\""
    depends_on:
      initializer:
        condition: service_completed_successfully
    secrets:
      - kafka-truststore
      - client2-keystore
    networks:
      - kafka-clients
    volumes:
      - ./client_properties/merger.properties:/app/client.properties
      - ./client_properties/log4j2.xml:/app/log4j2.xml
      - streams-state-dir:/app/streams-state

  extractor:
    <<: *scale-extractor
    profiles:
      - colext
      - full
    image: domrad/extractor
    depends_on:
      initializer:
        condition: service_completed_successfully
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    networks:
      - kafka-clients
    volumes:
      - ./client_properties/extractor.toml:/app/config.toml
      - ./extractor_data/:/app/data/

  config-manager:
    profiles:
      - configmanager
    image: domrad/config-manager
    depends_on:
      initializer:
        condition: service_completed_successfully
    user: root
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    networks:
      - kafka-clients
    volumes:
      - ./client_properties/:/var/domainradar_properties/
      - ./client_properties/config_manager.toml:/app/config.toml
      - /tmp/domrad_control.sock:/app/config_manager.sock

  standalone-input:
    profiles:
      - col
      - colext
    image: domrad/standalone-input
    depends_on:
      kafka1:
        condition: service_started
      mongo:
        condition: service_started
    secrets:
      - ca-cert
      - client2-cert
      - client2-key
    networks:
      - kafka-clients
      - databases
    volumes:
      - ./client_properties/standalone_input.toml:/app/config.toml


  # --- Databases ---

  postgres:
    profiles:
      - full
    image: docker.io/postgres:16
    restart: always
    secrets:
      - postgres_master_password
      - postgres_prefilter_password
      - postgres_controller_password
      - postgres_connect_password
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./db/postgres/init/:/docker-entrypoint-initdb.d/
      - ./db/postgres/postgres.conf:/etc/postgresql/postgresql.conf
    env_file:
      - ./db/postgres/postgres.env
    ports:
      - "31010:5432"
    networks:
      databases:
        ipv4_address: 192.168.101.2
        aliases: [ "postgres" ]
      outside-world:
        ipv4_address: 192.168.103.244

  mongo:
    image: docker.io/mongo:7
    command: --config /etc/mongod.conf
    restart: always
    secrets:
      - mongo_master_password
      - mongo_connect_password
    volumes:
      - mongo-data:/data/db
      - ./db/mongo/init/:/docker-entrypoint-initdb.d/
      - ./db/mongo/mongod.conf.yml:/etc/mongod.conf
    env_file:
      - ./db/mongo/mongo.env
    ports:
      - "31011:27017"
    networks:
      databases:
        ipv4_address: 192.168.101.3
        aliases: [ "mongo" ]
      outside-world:
        ipv4_address: 192.168.103.245

  mongo-domains-refresher: &mongo-aggregation-service
    profiles:
      - full
    build: 
      context: .
      dockerfile: ./dockerfiles/run_aggregation.Dockerfile
    restart: always
    depends_on:
      - mongo
    secrets:
      - mongo_master_password
    networks:
      databases:
        ipv4_address: 192.168.101.20
    environment:
      - AGGREGATION=results_without_classification_history.js
      - PERIOD_SEC=60

  mongo-raw-data-refresher:
    <<: *mongo-aggregation-service
    networks:
      databases:
        ipv4_address: 192.168.101.21
    environment:
      - AGGREGATION=all_raw_data.js
      - PERIOD_SEC=600

volumes:
  kafka1-single-log: {}
  postgres-data: {}
  mongo-data: {}
  kafka-connect-offsets: {}
  streams-state-dir: {}

networks:
  kafka-clients:
    driver: bridge
    enable_ipv6: false
    internal: true
    ipam:
      config:
        - subnet: 192.168.100.0/24
          gateway: 192.168.100.1
          ip_range: 192.168.100.128/25
  databases:
    driver: bridge
    enable_ipv6: false
    internal: true
    ipam:
      config:
        - subnet: 192.168.101.0/24
          gateway: 192.168.101.1
          ip_range: 192.168.101.128/25
  collectors:
    driver: bridge
    enable_ipv6: true
    ipam:
      config:
        - subnet: 192.168.102.0/24
          gateway: 192.168.102.1
          ip_range: 192.168.102.128/25
        - subnet: fd10:3456:789a:1::/64
  outside-world:
    driver: bridge
    enable_ipv6: false
    ipam:
      config:
        - subnet: 192.168.103.240/29
          gateway: 192.168.103.241
  kafka-outside-world:
    driver: bridge
    enable_ipv6: false
    ipam:
      config:
        - subnet: 192.168.103.248/29
          gateway: 192.168.103.249

secrets:
  kafka-truststore:
    file: ./secrets/kafka.truststore.jks
  kafka1-keystore:
    file: ./secrets/secrets_kafka1/kafka1.keystore.jks
  client1-keystore:
    file: ./secrets/secrets_client1/client1.keystore.jks
  client2-keystore:
    file: ./secrets/secrets_client2/client2.keystore.jks
  ca-cert:
    file: ./secrets/ca/ca-cert
  client2-cert:
    file: ./secrets/secrets_client2/client2-cert.pem
  client2-key:
    file: ./secrets/secrets_client2/client2-priv-key.pem

  # --- Databases ---
  postgres_master_password:
    file: ./db/postgres/master.secret
  postgres_prefilter_password:
    file: ./db/postgres/prefilter.secret
  postgres_controller_password:
    file: ./db/postgres/controller.secret
  postgres_connect_password:
    file: ./db/postgres/connect.secret
  mongo_master_password:
    file: ./db/mongo/master.secret
  mongo_connect_password:
    file: ./db/mongo/connect.secret
